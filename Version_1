import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler


# Basic settings
SEQ_LENGTH = 20  # How many past points to look at
PRED_LENGTH = 5  # How many future points to predict
HIDDEN_SIZE = 64  # Size of LSTM hidden layer
BATCH_SIZE = 32
LEARNING_RATE = 0.001
EPOCHS = 50


# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


# Load the data
print("Loading data...")
df = pd.read_excel('generated_training_data_2.xlsx')
flows = df['Flow Rate (gpm)'].values
pressures = df['Predicted Pressure (psiA)'].values


# Combine flow and pressure data
data = np.column_stack([flows, pressures])


# Normalize the data
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data)


# Create sequences for training
print("Creating sequences...")
X_data = []  # Input sequences
Y_data = []  # Target values


for i in range(len(data_normalized) - SEQ_LENGTH - PRED_LENGTH + 1):
    # Get SEQ_LENGTH points as input
    input_seq = data_normalized[i:i+SEQ_LENGTH]
    X_data.append(input_seq)
    
    # Get next PRED_LENGTH pressure values as target
    target_pressures = data_normalized[i+SEQ_LENGTH:i+SEQ_LENGTH+PRED_LENGTH, 1]
    Y_data.append(target_pressures)


X_data = np.array(X_data)
Y_data = np.array(Y_data)


# Split data into train and test (80/20 split)
split_point = int(0.8 * len(X_data))
X_train = torch.FloatTensor(X_data[:split_point])
Y_train = torch.FloatTensor(Y_data[:split_point])
X_test = torch.FloatTensor(X_data[split_point:])
Y_test = torch.FloatTensor(Y_data[split_point:])


print(f"Training samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")


# Define the LSTM + Dense model
class LSTMDenseModel(nn.Module):
    def __init__(self):
        super(LSTMDenseModel, self).__init__()
        # LSTM layer (this is a type of RNN)
        self.lstm = nn.LSTM(input_size=2, hidden_size=HIDDEN_SIZE, batch_first=True)
        
        # Dense layers (fully connected)
        self.dense1 = nn.Linear(HIDDEN_SIZE, 32)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(32, PRED_LENGTH)
        
    def forward(self, x):
        # Pass through LSTM
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # Use the last hidden state
        last_hidden = hidden[-1]
        
        # Pass through dense layers
        x = self.dense1(last_hidden)
        x = self.relu(x)
        predictions = self.dense2(x)
        
        return predictions


# Create model, loss function, and optimizer
model = LSTMDenseModel().to(device)
loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)


# Print model info
total_params = sum(p.numel() for p in model.parameters())
print(f"\nModel has {total_params:,} parameters")


# Training loop
print("\nStarting training...")
train_losses = []

for epoch in range(EPOCHS):
    total_loss = 0
    num_batches = 0
    
    # Process data in batches
    for i in range(0, len(X_train), BATCH_SIZE):
        # Get batch
        batch_X = X_train[i:i+BATCH_SIZE].to(device)
        batch_Y = Y_train[i:i+BATCH_SIZE].to(device)
        
        # Forward pass
        predictions = model(batch_X)
        loss = loss_function(predictions, batch_Y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    # Calculate average loss
    avg_loss = total_loss / num_batches
    train_losses.append(avg_loss)
    
    # Print progress every 10 epochs
    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}')


# Test the model
print("\nTesting model...")
model.eval()  # Set to evaluation mode


# Make predictions on test data
all_predictions = []
all_targets = []


with torch.no_grad():  # Don't calculate gradients during testing
    for i in range(0, len(X_test), BATCH_SIZE):
        batch_X = X_test[i:i+BATCH_SIZE].to(device)
        batch_Y = Y_test[i:i+BATCH_SIZE]
        
        predictions = model(batch_X)
        all_predictions.append(predictions.cpu().numpy())
        all_targets.append(batch_Y.numpy())


# Combine all predictions and targets
predictions_array = np.vstack(all_predictions)
targets_array = np.vstack(all_targets)


# Denormalize to get actual pressure values
pressure_mean = scaler.mean_[1]
pressure_std = scaler.scale_[1]

predictions_actual = predictions_array * pressure_std + pressure_mean
targets_actual = targets_array * pressure_std + pressure_mean


# Calculate error metrics
mse = np.mean((predictions_array - targets_array)**2)
mae = np.mean(np.abs(predictions_actual - targets_actual))
rmse = np.sqrt(np.mean((predictions_actual - targets_actual)**2))

print(f"\nTest Results:")
print(f"MSE (normalized): {mse:.4f}")
print(f"MAE: {mae:.2f} psiA")
print(f"RMSE: {rmse:.2f} psiA")


# Plot 1: Training loss
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(train_losses)
plt.title('Training Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)


# Plot 2: Predictions vs Actual
plt.subplot(1, 2, 2)
plt.plot(targets_actual[:100, 0], label='Actual Pressure', alpha=0.7)
plt.plot(predictions_actual[:100, 0], label='Predicted Pressure', alpha=0.7)
plt.title('LSTM + Dense: Pressure Predictions')
plt.xlabel('Test Sample')
plt.ylabel('Pressure (psiA)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nDone!")
