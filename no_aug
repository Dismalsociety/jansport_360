# SECTION 1 : Imports and Libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import warnings
warnings.filterwarnings('ignore')


# SECTION 2 : Basic settings
SEQ_LENGTH = 15  # Sequence length
PRED_LENGTH = 5  # How many future points to predict
BATCH_SIZE = 32  # Increased from 1 for better gradient estimates
LEARNING_RATE = 0.001  
EPOCHS = 200  # Reduced since we'll use early stopping
DROPOUT_RATE = 0.3  # Dropout for regularization
WEIGHT_DECAY = 1e-5  # L2 regularization


# SECTION 3 : Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


# SECTION 4 : Load the data
print("Loading data...")
df = pd.read_excel('generated_training_data.xlsx')
flows = df['Flow Rate (gpm)'].values
pressures = df['Predicted Pressure (psiA)'].values

print(f"Pressure range: {pressures.min():.1f} to {pressures.max():.1f}")
print(f"Flow range: {flows.min():.1f} to {flows.max():.1f}")


# SECTION 5 : Normalize data
flow_scaler = MinMaxScaler(feature_range=(-1, 1))
pressure_scaler = MinMaxScaler(feature_range=(-1, 1))

flows_normalized = flow_scaler.fit_transform(flows.reshape(-1, 1)).flatten()
pressures_normalized = pressure_scaler.fit_transform(pressures.reshape(-1, 1)).flatten()

# Combine normalized data
data_normalized = np.column_stack([flows_normalized, pressures_normalized])


# SECTION 6 : Create sequences with PROPER temporal splitting
print("Creating sequences with temporal awareness...")

# CRITICAL: Split data BEFORE creating sequences to avoid data leakage
total_samples = len(data_normalized)
train_size = int(0.7 * total_samples)
val_size = int(0.15 * total_samples)

# Temporal split - no shuffling for time series!
train_data = data_normalized[:train_size]
val_data = data_normalized[train_size:train_size + val_size]
test_data = data_normalized[train_size + val_size:]

print(f"Train data: {len(train_data)} samples")
print(f"Val data: {len(val_data)} samples")
print(f"Test data: {len(test_data)} samples")

def create_sequences(data, seq_length, pred_length):
    X, Y = [], []
    for i in range(len(data) - seq_length - pred_length + 1):
        X.append(data[i:i+seq_length])
        Y.append(data[i+seq_length:i+seq_length+pred_length])
    return np.array(X), np.array(Y)

# Create sequences for each split
X_train, Y_train = create_sequences(train_data, SEQ_LENGTH, PRED_LENGTH)
X_val, Y_val = create_sequences(val_data, SEQ_LENGTH, PRED_LENGTH)
X_test, Y_test = create_sequences(test_data, SEQ_LENGTH, PRED_LENGTH)

print(f"\nSequences created:")
print(f"Training: X={X_train.shape}, Y={Y_train.shape}")
print(f"Validation: X={X_val.shape}, Y={Y_val.shape}")
print(f"Test: X={X_test.shape}, Y={Y_test.shape}")

# Convert to tensors
X_train = torch.FloatTensor(X_train).to(device)
Y_train = torch.FloatTensor(Y_train).to(device)
X_val = torch.FloatTensor(X_val).to(device)
Y_val = torch.FloatTensor(Y_val).to(device)
X_test = torch.FloatTensor(X_test).to(device)
Y_test = torch.FloatTensor(Y_test).to(device)


# SECTION 7 : Create DataLoaders for batch processing
train_dataset = TensorDataset(X_train, Y_train)
val_dataset = TensorDataset(X_val, Y_val)
test_dataset = TensorDataset(X_test, Y_test)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)


# SECTION 8 : Define Improved LSTM Model with Regularization
class ImprovedLSTMModel(nn.Module):
    def __init__(self, input_size=2, hidden_dim=64, output_size=2, 
                 num_layers=3, dropout=0.3, pred_length=5):
        super(ImprovedLSTMModel, self).__init__()
        self.pred_length = pred_length
        self.hidden_dim = hidden_dim
        
        # Dense layer BEFORE LSTM (feature extraction)
        self.input_dense = nn.Sequential(
            nn.Linear(input_size, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Stacked LSTM layers with dropout
        self.lstm_layers = nn.ModuleList()
        
        # First LSTM layer
        self.lstm_layers.append(nn.LSTM(
            hidden_dim, 64, 
            batch_first=True, 
            dropout=0 if num_layers == 1 else dropout
        ))
        
        # Middle LSTM layers with increasing then decreasing size
        lstm_sizes = [64, 128, 256, 128, 64]
        for i in range(len(lstm_sizes) - 1):
            self.lstm_layers.append(nn.LSTM(
                lstm_sizes[i], lstm_sizes[i+1],
                batch_first=True,
                dropout=dropout if i < len(lstm_sizes) - 2 else 0
            ))
        
        # Dropout layer for additional regularization
        self.dropout = nn.Dropout(dropout)
        
        # Dense layers AFTER LSTM (output projection)
        self.output_layers = nn.Sequential(
            nn.Linear(64, 32),
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, output_size)
        )
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights using Xavier/He initialization"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.constant_(param, 0)
    
    def forward(self, x):
        batch_size = x.size(0)
        seq_len = x.size(1)
        
        # Apply input dense layer to each timestep
        x_reshaped = x.reshape(-1, x.size(-1))
        x_dense = self.input_dense(x_reshaped)
        x = x_dense.reshape(batch_size, seq_len, -1)
        
        # Pass through stacked LSTM layers
        for lstm in self.lstm_layers:
            x, _ = lstm(x)
            x = self.dropout(x)
        
        # Take last pred_length timesteps
        x = x[:, -self.pred_length:, :]
        
        # Apply output dense layers to each timestep
        x_reshaped = x.reshape(-1, x.size(-1))
        output = self.output_layers(x_reshaped)
        output = output.reshape(batch_size, self.pred_length, -1)
        
        return output


# SECTION 9 : Define custom dip-aware loss function
def dip_aware_loss(predictions, targets, pressure_scaler, dip_threshold=-5.0):
    """Weighted loss that penalizes missing pressure dips more heavily"""
    # Base MSE loss
    mse_loss = F.mse_loss(predictions, targets)
    
    # Extract pressure predictions and targets
    pred_pressure = predictions[:, :, 1]
    target_pressure = targets[:, :, 1]
    
    # Denormalize to check for dips
    target_pressure_np = target_pressure.cpu().detach().numpy()
    target_actual = pressure_scaler.inverse_transform(
        target_pressure_np.reshape(-1, 1)
    ).reshape(target_pressure_np.shape)
    
    # Identify dip regions
    dip_mask = torch.tensor(target_actual < dip_threshold, device=predictions.device, dtype=torch.float)
    
    # Calculate weighted loss for pressure only
    pressure_loss = (pred_pressure - target_pressure) ** 2
    weighted_pressure_loss = pressure_loss * (1 + 2 * dip_mask)  # 3x weight on dips
    
    # Combine losses
    total_loss = mse_loss + 0.5 * weighted_pressure_loss.mean()
    
    return total_loss


# SECTION 10 : Training class with early stopping
class Trainer:
    def __init__(self, model, lr=0.001, weight_decay=1e-5, patience=15):
        self.model = model
        self.optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=lr, 
            weight_decay=weight_decay
        )
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )
        self.patience = patience
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.best_model_state = None
        self.train_losses = []
        self.val_losses = []
    
    def train_epoch(self, train_loader):
        self.model.train()
        epoch_loss = 0
        n_batches = 0
        
        for batch_x, batch_y in train_loader:
            self.optimizer.zero_grad()
            
            predictions = self.model(batch_x)
            loss = dip_aware_loss(predictions, batch_y, pressure_scaler)
            
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            epoch_loss += loss.item()
            n_batches += 1
        
        return epoch_loss / n_batches
    
    def validate(self, val_loader):
        self.model.eval()
        val_loss = 0
        n_batches = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                predictions = self.model(batch_x)
                loss = dip_aware_loss(predictions, batch_y, pressure_scaler)
                val_loss += loss.item()
                n_batches += 1
        
        return val_loss / n_batches
    
    def fit(self, train_loader, val_loader, epochs):
        print("\nTraining with early stopping...")
        print("-" * 60)
        
        for epoch in range(epochs):
            # Training
            train_loss = self.train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            # Validation
            val_loss = self.validate(val_loader)
            self.val_losses.append(val_loss)
            
            # Learning rate scheduling
            self.scheduler.step(val_loss)
            
            # Early stopping check
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.best_model_state = self.model.state_dict().copy()
            else:
                self.patience_counter += 1
            
            # Print progress
            print(f'Epoch {epoch+1:3d}/{epochs} | '
                  f'Train Loss: {train_loss:.4f} | '
                  f'Val Loss: {val_loss:.4f} | '
                  f'Best Val: {self.best_val_loss:.4f} | '
                  f'Patience: {self.patience_counter}/{self.patience}')
            
            # Early stopping
            if self.patience_counter >= self.patience:
                print(f'\nEarly stopping triggered at epoch {epoch+1}')
                self.model.load_state_dict(self.best_model_state)
                break
        
        return self.train_losses, self.val_losses


# SECTION 11 : Create and train model
model = ImprovedLSTMModel(
    input_size=2,
    hidden_dim=64,
    output_size=2,
    num_layers=6,
    dropout=DROPOUT_RATE,
    pred_length=PRED_LENGTH
).to(device)

total_params = sum(p.numel() for p in model.parameters())
print(f"\nModel has {total_params:,} parameters")
print("Architecture: Input Dense -> 6 LSTM layers (64->128->256->128->64->2) -> Output Dense")
print(f"Dropout rate: {DROPOUT_RATE}")
print(f"Weight decay (L2): {WEIGHT_DECAY}")

# Train the model
trainer = Trainer(model, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, patience=15)
train_losses, val_losses = trainer.fit(train_loader, val_loader, EPOCHS)


# SECTION 12 : Evaluate model
print("\nEvaluating model on test set...")
model.eval()

all_predictions = []
all_targets = []

with torch.no_grad():
    for batch_x, batch_y in test_loader:
        predictions = model(batch_x)
        all_predictions.append(predictions.cpu().numpy())
        all_targets.append(batch_y.cpu().numpy())

predictions = np.concatenate(all_predictions, axis=0)
targets = np.concatenate(all_targets, axis=0)

# Extract pressure predictions
predictions_pressure = predictions[:, :, 1]
targets_pressure = targets[:, :, 1]

# Denormalize
predictions_actual = pressure_scaler.inverse_transform(
    predictions_pressure.reshape(-1, 1)
).reshape(predictions_pressure.shape)

targets_actual = pressure_scaler.inverse_transform(
    targets_pressure.reshape(-1, 1)
).reshape(targets_pressure.shape)

# Calculate metrics
correlation = np.corrcoef(predictions_actual.flatten(), targets_actual.flatten())[0, 1]
mse = np.mean((predictions_actual - targets_actual) ** 2)
rmse = np.sqrt(mse)
mae = np.mean(np.abs(predictions_actual - targets_actual))

# Dip detection metrics
dip_mask = targets_actual < -5.0
if dip_mask.any():
    dip_mse = np.mean((predictions_actual[dip_mask] - targets_actual[dip_mask]) ** 2)
    dip_rmse = np.sqrt(dip_mse)
    print(f"Dip regions RMSE: {dip_rmse:.2f} psiA")
    print(f"Number of dip samples: {dip_mask.sum()}")

print(f"\nOverall Metrics:")
print(f"Correlation: {correlation:.3f}")
print(f"RMSE: {rmse:.2f} psiA")
print(f"MAE: {mae:.2f} psiA")

# Fidelity scores
errors = np.abs(predictions_actual - targets_actual)
mean_error = np.mean(errors)
pressure_range = np.max(targets_actual) - np.min(targets_actual)
accuracy_score = max(0, min(100, (1 - mean_error / pressure_range) * 100))
dynamics_score = correlation * 100

print(f"\nFidelity Scores:")
print(f"Accuracy: {accuracy_score:.1f}% (value closeness)")
print(f"Dynamics: {dynamics_score:.1f}% (pattern capture)")
print(f"Overall: {(accuracy_score + dynamics_score) / 2:.1f}%")


# SECTION 13 : Visualization
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Plot 1: Training history
axes[0, 0].plot(train_losses, label='Train Loss', alpha=0.7, linewidth=2)
axes[0, 0].plot(val_losses, label='Val Loss', alpha=0.7, linewidth=2)
axes[0, 0].set_title('Training History (with Early Stopping)')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].set_yscale('log')

# Plot 2: Learning curves (zoomed)
recent_epochs = min(50, len(train_losses))
axes[0, 1].plot(train_losses[-recent_epochs:], label='Train Loss', linewidth=2)
axes[0, 1].plot(val_losses[-recent_epochs:], label='Val Loss', linewidth=2)
axes[0, 1].set_title(f'Last {recent_epochs} Epochs (Detail)')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Time series predictions
n_show = min(200, len(targets_actual))
axes[0, 2].plot(targets_actual[:n_show, 0], 'b-', label='Actual', linewidth=2)
axes[0, 2].plot(predictions_actual[:n_show, 0], 'r-', label='Predicted', alpha=0.8)
axes[0, 2].axhline(y=-5.0, color='g', linestyle='--', alpha=0.5, label='Dip Threshold')
axes[0, 2].set_title('Pressure Predictions - First Step')
axes[0, 2].set_xlabel('Test Sample')
axes[0, 2].set_ylabel('Pressure (psiA)')
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# Plot 4: Scatter plot
axes[1, 0].scatter(targets_actual.flatten(), predictions_actual.flatten(), 
                   alpha=0.3, s=1, c='blue')
if dip_mask.any():
    axes[1, 0].scatter(targets_actual.flatten()[dip_mask.flatten()], 
                       predictions_actual.flatten()[dip_mask.flatten()], 
                       alpha=0.8, s=2, c='red', label='Dip regions')
min_val = min(targets_actual.min(), predictions_actual.min())
max_val = max(targets_actual.max(), predictions_actual.max())
axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, alpha=0.5)
axes[1, 0].axvline(x=-5.0, color='g', linestyle='--', alpha=0.5)
axes[1, 0].set_title(f'Prediction Accuracy (R={correlation:.3f})')
axes[1, 0].set_xlabel('Actual Pressure (psiA)')
axes[1, 0].set_ylabel('Predicted Pressure (psiA)')
axes[1, 0].grid(True, alpha=0.3)
if dip_mask.any():
    axes[1, 0].legend()

# Plot 5: Residuals histogram
residuals = predictions_actual.flatten() - targets_actual.flatten()
axes[1, 1].hist(residuals, bins=50, alpha=0.7, color='blue', edgecolor='black')
axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)
axes[1, 1].set_title(f'Residuals Distribution (μ={np.mean(residuals):.2f}, σ={np.std(residuals):.2f})')
axes[1, 1].set_xlabel('Prediction Error (psiA)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].grid(True, alpha=0.3)

# Plot 6: Multi-step predictions
for step in range(min(3, PRED_LENGTH)):  # Show first 3 steps
    alpha = 0.8 - step * 0.2
    axes[1, 2].plot(targets_actual[:50, step], 'b-', alpha=alpha, 
                    linewidth=2-step*0.3, label=f'Actual (t+{step+1})' if step == 0 else '')
    axes[1, 2].plot(predictions_actual[:50, step], 'r-', alpha=alpha, 
                    linewidth=2-step*0.3, label=f'Predicted (t+{step+1})' if step == 0 else '')

axes[1, 2].axhline(y=-5.0, color='g', linestyle='--', alpha=0.5, label='Dip Threshold')
axes[1, 2].set_title(f'Multi-Step Ahead Predictions')
axes[1, 2].set_xlabel('Test Sample')
axes[1, 2].set_ylabel('Pressure (psiA)')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("Model training complete!")
print("Key improvements implemented:")
print("1. ✓ Temporal data splitting (no data leakage)")
print("2. ✓ Dense layers before and after LSTM")
print("3. ✓ Dropout regularization (0.3)")
print("4. ✓ L2 weight decay (1e-5)")
print("5. ✓ Batch normalization in dense layers")
print("6. ✓ Gradient clipping")
print("7. ✓ Early stopping with patience")
print("8. ✓ Learning rate scheduling")
print("9. ✓ Proper batch processing (batch_size=32)")
print("10. ✓ Orthogonal LSTM weight initialization")
print("="*60)
