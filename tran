# Define Model 
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

        # Input layer
        self.input_layer = nn.Linear(2, 64)

        # Transformer 
        self.transformer = nn.TransformerEncoderLayer(
            d_model=64,
            nhead=4,
            dim_feedforward=128,
            batch_first=True)

        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size = 64, 
            hidden_size = 128,
            bidirectional = True, 
            batch_first=True )

        # Output layers 
        self.fc1 = nn.Linear(256, 32)
        self.fc2 = nn.Linear(32, predict_length)
        self.relu = nn.ReLU()

    def forward(self, x):
        
        x = self.input_layer(x)
        x = self.transformer(x)
        lstm_out, (hidden, cell) = self.lstm(x)
        hidden_forward = hidden[0]
        hidden_backward = hidden[1]
        last_hidden = torch.cat((hidden_forward, hidden_backward), dim=1)
        x = self.fc1(last_hidden)
        x = self.relu(x)
        x = self.fc2(x)

        return x

model = Model().to(device)
