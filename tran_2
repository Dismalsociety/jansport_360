import torch
import torch.nn as nn
import matplotlib.pyplot as plt 
import pandas as pd 
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr
import math

# Parameters
sequence_length = 10    # How many past points to use
predict_length = 5     # How many future points to predict
batch_size = 32     
learning_rate = 0.001 
epochs = 100

# Use GPU instead of CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load data
print("Loading data...")
df = pd.read_excel('generated_training_data_2.xlsx')
flow = df['FLow Rate'].values 
pressure = df['Pressure'].values 

# Combine our two features together 
data = np.column_stack([flow, pressure])

#Normalize data
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data)

# Create sliding windows for training 
print("Creating sequences...")
X_data = []
Y_data = []

for i in range(len(data_normalized) - sequence_length - predict_length + 1):
    
    input_seq = data_normalized[i:i+sequence_length]
    X_data.append(input_seq)

    target = data_normalized[i+sequence_length:i+sequence_length+predict_length, 1]
    Y_data.append(target)

X_data = np.array(X_data)
Y_data = np.array(Y_data)

# Split into train and test
split = int(0.8 * len(X_data))
X_train = torch.FloatTensor(X_data[:split])
Y_train = torch.FloatTensor(Y_data[:split])
X_test = torch.FloatTensor(X_data[split:])
Y_test = torch.FloatTensor(Y_data[split:])

print(f"Training samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")

# Define Model 
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

        # Input layer
        self.input_layer = nn.Linear(2, 64)

        # Transformer 
        self.transformer = nn.TransformerEncoderLayer(
            d_model=64,
            nhead=4,
            dim_feedforward=128,
            batch_first=True)

        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size = 64, 
            hidden_size = 128,
            bidirectional = True,  # Added bidirectional
            batch_first=True )

        # Output layers - note: input size is now 256 (128*2) due to bidirectional
        self.fc1 = nn.Linear(256, 32)
        self.fc2 = nn.Linear(32, predict_length)
        self.relu = nn.ReLU()

    def forward(self, x):
        
        x = self.input_layer(x)
        x = self.transformer(x)
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # For bidirectional LSTM, concatenate the last hidden states from both directions
        # hidden shape is (num_layers * num_directions, batch, hidden_size)
        # We need to concatenate the forward and backward hidden states
        hidden_forward = hidden[0]
        hidden_backward = hidden[1]
        last_hidden = torch.cat((hidden_forward, hidden_backward), dim=1)
        
        x = self.fc1(last_hidden)
        x = self.relu(x)
        x = self.fc2(x)

        return x

model = Model().to(device)

# Loss function (MSE) and optimizer (Adam)
MSE = nn.MSELoss()
Adam = torch.optim.Adam(model.parameters(), lr = learning_rate)

# Learning Rate Schedulers - Choose one:

# Option 1: ReduceLROnPlateau - Reduces LR when loss plateaus
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    Adam, 
    mode='min',          # Minimize loss
    factor=0.5,          # Multiply LR by 0.5
    patience=5,          # Wait 5 epochs before reducing
    verbose=True,        # Print when LR changes
    min_lr=1e-6         # Don't go below this LR
)

# Option 2: CosineAnnealingLR - Smooth cosine decay
# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
#     Adam,
#     T_max=epochs,        # Period of cosine wave
#     eta_min=1e-6        # Minimum LR
# )

# Option 3: StepLR - Step down at fixed intervals
# scheduler = torch.optim.lr_scheduler.StepLR(
#     Adam,
#     step_size=30,       # Reduce every 30 epochs
#     gamma=0.1           # Multiply LR by 0.1
# )

# Option 4: OneCycleLR - Cycle up then down (often best for transformers)
# total_steps = epochs * (len(X_train) // batch_size)
# scheduler = torch.optim.lr_scheduler.OneCycleLR(
#     Adam,
#     max_lr=0.01,        # Peak learning rate
#     total_steps=total_steps,
#     pct_start=0.3,      # Warmup for 30% of training
#     anneal_strategy='cos'
# )

# Training 
print("\nTraining...")
train_losses = []
val_losses = []
learning_rates = []

# Create validation set from last 20% of training data
val_split = int(0.8 * len(X_train))
X_train_sub = X_train[:val_split]
Y_train_sub = Y_train[:val_split]
X_val = X_train[val_split:]
Y_val = Y_train[val_split:]

for epoch in range(epochs):
    model.train()
    total_loss = 0
    num_batches = 0

    # Train on batches
    for i in range(0, len(X_train_sub), batch_size):
        # Get batch
        batch_x = X_train_sub[i:i+batch_size].to(device)
        batch_y = Y_train_sub[i:i+batch_size].to(device)
    
        # Forward pass
        outputs = model(batch_x)
        loss = MSE(outputs, batch_y)
        
        # Backward pass
        Adam.zero_grad()
        loss.backward()
        Adam.step()
        
        # Step scheduler if using OneCycleLR (needs step per batch)
        # if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):
        #     scheduler.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    # Average training loss
    avg_train_loss = total_loss / num_batches
    train_losses.append(avg_train_loss)
    
    # Validation loss
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val.to(device))
        val_loss = MSE(val_outputs, Y_val.to(device))
        val_losses.append(val_loss.item())
    
    # Step scheduler (for epoch-based schedulers)
    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
        scheduler.step(val_loss)  # Use validation loss for plateau detection
    elif not isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):
        scheduler.step()
    
    # Track learning rate
    current_lr = Adam.param_groups[0]['lr']
    learning_rates.append(current_lr)
    
    # Print every 10 epochs
    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}')

# Testing
print("\nTesting...")
model.eval()

# Predictions 
all_predictions = []
all_actuals = []

with torch.no_grad():
    for i in range(0, len(X_test), batch_size):
        batch_x = X_test[i:i+batch_size].to(device)
        batch_y = Y_test[i:i+batch_size]
        
        predictions = model(batch_x)
        
        #Store results here
        all_predictions.append(predictions.cpu().numpy())
        all_actuals.append(batch_y.numpy())

predictions = np.vstack(all_predictions)
actuals = np.vstack(all_actuals)

# Convert values back to original scale
pressure_mean = scaler.mean_[1]
pressure_std = scaler.scale_[1]

predictions_real = predictions * pressure_std + pressure_mean
actuals_real = actuals * pressure_std + pressure_mean

# Calculate fidelity metrics
correlation, p_value = pearsonr(predictions_real.flatten(), actuals_real.flatten())
ss_res = np.sum((actuals_real - predictions_real) ** 2)
ss_tot = np.sum((actuals_real - np.mean(actuals_real)) ** 2)
r_squared = 1 - (ss_res / ss_tot)

print(f"\nTrend Fidelity:")
print(f"  Correlation: {correlation:.4f}")
print(f"  R-squared: {r_squared:.4f}")

# Make plots
plt.figure(figsize=(15, 5))

# Plot 1: Training and validation loss
plt.subplot(1, 3, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Plot 2: Learning rate schedule
plt.subplot(1, 3, 2)
plt.plot(learning_rates)
plt.title('Learning Rate Schedule')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.yscale('log')  # Log scale for better visualization
plt.grid(True)

# Plot 3: Predictions vs Actual
plt.subplot(1, 3, 3)
plt.plot(actuals_real[:, 0], label='Actual')
plt.plot(predictions_real[:, 0], label='Predicted')
plt.title('Predictions vs Actual')
plt.xlabel('Test Sample')
plt.ylabel('Pressure (psiA)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

print("\nDone!")
