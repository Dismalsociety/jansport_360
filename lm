# SECTION 1 : Imports
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')


# SECTION 2 : Basic settings
SEQ_LENGTH = 50  # How many past steps to use
BATCH_SIZE = 32
LEARNING_RATE = 0.001
EPOCHS = 100
HIDDEN_SIZE = 64
NUM_LAYERS = 2
DROPOUT = 0.2


# SECTION 3 : Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


# SECTION 4 : Load data
# df = pd.read_excel('.xlsx')
df = pd.read_csv('swash_plate_pump_data (7).csv')
flow_rate = df['Flow Rate (GPM)'].values
pressure = df['Pressure (PSI)'].values


# SECTION 5 : Normalize data
scaler_X = StandardScaler()
scaler_y = StandardScaler()

flow_normalized = scaler_X.fit_transform(flow_rate.reshape(-1, 1)).flatten()
pressure_normalized = scaler_y.fit_transform(pressure.reshape(-1, 1)).flatten()

# Combine data
data = np.column_stack((flow_normalized, pressure_normalized))


# SECTION 6 : Create sequences
def create_sequences(data, seq_length=50, stride=1):
    X, y = [], []
    for i in range(0, len(data) - seq_length, stride):
        X.append(data[i:i+seq_length, 0])  # Flow sequences
        y.append(data[i+seq_length, 1])     # Next pressure value
    return np.array(X), np.array(y)

X, y = create_sequences(data, SEQ_LENGTH)


# SECTION 7 : Split data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)

# Convert to tensors
X_train = torch.FloatTensor(X_train).to(device)
y_train = torch.FloatTensor(y_train).to(device)
X_val = torch.FloatTensor(X_val).to(device)
y_val = torch.FloatTensor(y_val).to(device)
X_test = torch.FloatTensor(X_test).to(device)
y_test = torch.FloatTensor(y_test).to(device)


# SECTION 8 : Define LSTM Model with Dense Layers
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        
        # LSTM layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)
        
        # Two dense layers
        self.dense1 = nn.Linear(hidden_size, 32)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.dense2 = nn.Linear(32, 1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]
        
        x = self.dense1(last_output)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.dense2(x)
        
        return x


# SECTION 9 : Initialize model
model = LSTMModel(input_size=1, hidden_size=HIDDEN_SIZE, 
                 num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)


# SECTION 10 : Training function
def train_model(model, X_train, y_train, X_val, y_val, epochs, batch_size):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        # Training
        model.train()
        epoch_loss = 0
        num_batches = 0
        
        # Process in batches
        for i in range(0, len(X_train), batch_size):
            batch_X = X_train[i:i+batch_size]
            batch_y = y_train[i:i+batch_size]
            
            # Add dimension for LSTM input
            batch_X = batch_X.unsqueeze(-1)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs.squeeze(), batch_y)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        avg_train_loss = epoch_loss / num_batches
        train_losses.append(avg_train_loss)
        
        # Validation
        model.eval()
        with torch.no_grad():
            X_val_input = X_val.unsqueeze(-1)
            val_outputs = model(X_val_input)
            val_loss = criterion(val_outputs.squeeze(), y_val)
            val_losses.append(val_loss.item())
        
        scheduler.step(val_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}")
    
    return train_losses, val_losses


# SECTION 11 : Train the model
train_losses, val_losses = train_model(model, X_train, y_train, X_val, y_val, 
                                       epochs=EPOCHS, batch_size=BATCH_SIZE)


# SECTION 12 : Evaluate model
model.eval()

with torch.no_grad():
    X_test_input = X_test.unsqueeze(-1)
    predictions = model(X_test_input).squeeze().cpu().numpy()
    actuals = y_test.cpu().numpy()

# Denormalize
predictions_orig = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()
actuals_orig = scaler_y.inverse_transform(actuals.reshape(-1, 1)).flatten()

# Calculate metrics
mse = np.mean((predictions_orig - actuals_orig) ** 2)
rmse = np.sqrt(mse)

# Calculate R-squared for fidelity
ss_res = np.sum((actuals_orig - predictions_orig) ** 2)
ss_tot = np.sum((actuals_orig - np.mean(actuals_orig)) ** 2)
r2 = 1 - (ss_res / ss_tot)
fidelity = r2 * 100


# SECTION 13 : Print results
print("\n" + "="*50)
print("RESULTS")
print("="*50)
print(f"Fidelity Score: {fidelity:.2f}%")
print(f"RMSE: {rmse:.2f} PSI")
print("="*50)


# SECTION 14 : Visualization
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot 1: Training history
axes[0, 0].plot(train_losses, label='Train Loss', alpha=0.7)
axes[0, 0].plot(val_losses, label='Val Loss', alpha=0.7)
axes[0, 0].set_title('Training History')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Predictions vs Actuals
sample_size = min(500, len(predictions_orig))
axes[0, 1].plot(actuals_orig[:sample_size], 'b-', label='Actual', alpha=0.7)
axes[0, 1].plot(predictions_orig[:sample_size], 'r-', label='Predicted', alpha=0.7)
axes[0, 1].set_title('Pressure Predictions')
axes[0, 1].set_xlabel('Test Sample')
axes[0, 1].set_ylabel('Pressure (PSI)')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Scatter plot
axes[1, 0].scatter(actuals_orig, predictions_orig, alpha=0.5, s=1, c='blue')
min_val = min(actuals_orig.min(), predictions_orig.min())
max_val = max(actuals_orig.max(), predictions_orig.max())
axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
axes[1, 0].set_title(f'Prediction Accuracy (RÂ²: {r2:.3f})')
axes[1, 0].set_xlabel('Actual Pressure (PSI)')
axes[1, 0].set_ylabel('Predicted Pressure (PSI)')
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Residuals
residuals = actuals_orig - predictions_orig
axes[1, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
axes[1, 1].set_title('Residuals Distribution')
axes[1, 1].set_xlabel('Residual (Actual - Predicted)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].axvline(x=0, color='r', linestyle='--', alpha=0.5)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
