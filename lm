# SECTION 1 : Imports and Libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings('ignore')


# SECTION 2 : Basic settings
SEQ_LENGTH = 15  # Sequence length
PRED_LENGTH = 5  # How many future points to predict
BATCH_SIZE = 1  # As requested in original
LEARNING_RATE = 0.001  # Adam default
EPOCHS = 1000  # As requested in original


# SECTION 3 : Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


# SECTION 4 : Load the data
print("Loading data...")
df = pd.read_excel('generated_training_data.xlsx')
flows = df['Flow Rate (gpm)'].values
pressures = df['Predicted Pressure (psiA)'].values

print(f"Pressure range: {pressures.min():.1f} to {pressures.max():.1f}")
print(f"Flow range: {flows.min():.1f} to {flows.max():.1f}")


# SECTION 5 : Normalize data
flow_scaler = MinMaxScaler(feature_range=(-1, 1))
pressure_scaler = MinMaxScaler(feature_range=(-1, 1))

flows_normalized = flow_scaler.fit_transform(flows.reshape(-1, 1)).flatten()
pressures_normalized = pressure_scaler.fit_transform(pressures.reshape(-1, 1)).flatten()

# Combine normalized data
data_normalized = np.column_stack([flows_normalized, pressures_normalized])


# SECTION 6 : Create sequences with PROPER temporal splitting
print("Creating sequences with temporal awareness...")

# CRITICAL: Split data BEFORE creating sequences to avoid data leakage
total_samples = len(data_normalized)
train_size = int(0.7 * total_samples)
val_size = int(0.15 * total_samples)

# Temporal split - no shuffling for time series!
train_data = data_normalized[:train_size]
val_data = data_normalized[train_size:train_size + val_size]
test_data = data_normalized[train_size + val_size:]

print(f"Train data: {len(train_data)} samples")
print(f"Val data: {len(val_data)} samples")
print(f"Test data: {len(test_data)} samples")

def create_sequences(data, seq_length, pred_length):
    X, Y = [], []
    for i in range(len(data) - seq_length - pred_length + 1):
        X.append(data[i:i+seq_length])
        Y.append(data[i+seq_length:i+seq_length+pred_length])
    return np.array(X), np.array(Y)

# Create sequences for each split
X_train, Y_train = create_sequences(train_data, SEQ_LENGTH, PRED_LENGTH)
X_val, Y_val = create_sequences(val_data, SEQ_LENGTH, PRED_LENGTH)
X_test, Y_test = create_sequences(test_data, SEQ_LENGTH, PRED_LENGTH)

print(f"\nSequences created:")
print(f"Training: X={X_train.shape}, Y={Y_train.shape}")
print(f"Validation: X={X_val.shape}, Y={Y_val.shape}")
print(f"Test: X={X_test.shape}, Y={Y_test.shape}")

# Convert to tensors
X_train = torch.FloatTensor(X_train).to(device)
Y_train = torch.FloatTensor(Y_train).to(device)
X_val = torch.FloatTensor(X_val).to(device)
Y_val = torch.FloatTensor(Y_val).to(device)
X_test = torch.FloatTensor(X_test).to(device)
Y_test = torch.FloatTensor(Y_test).to(device)


# SECTION 7 : NO DataLoaders - use tensors directly like original
# Just keep as tensors for direct indexing like original code


# SECTION 8 : Define Sequential-style LSTM Model (matching original)
class SequentialLSTMModel(nn.Module):
    def __init__(self, input_size=2, output_size=2):
        super(SequentialLSTMModel, self).__init__()
        
        # Following your original architecture exactly
        self.lstm1 = nn.LSTM(input_size, 64, batch_first=True)
     
    def forward(self, x):
        # Pass through LSTM layer
        out, _ = self.lstm1(x)
     
        # Return only the last PRED_LENGTH outputs
        return out[:, -PRED_LENGTH:, :]


# SECTION 9 : Define custom dip-aware loss function
def dip_aware_loss(predictions, targets, pressure_scaler, dip_threshold=-5.0):
    """Weighted loss that penalizes missing pressure dips more heavily"""
    # Base MSE loss
    mse_loss = F.mse_loss(predictions, targets)
    
    # Extract pressure predictions and targets
    pred_pressure = predictions[:, :, 1]
    target_pressure = targets[:, :, 1]
    
    # Denormalize to check for dips
    target_pressure_np = target_pressure.cpu().detach().numpy()
    target_actual = pressure_scaler.inverse_transform(
        target_pressure_np.reshape(-1, 1)
    ).reshape(target_pressure_np.shape)
    
    # Identify dip regions
    dip_mask = torch.tensor(target_actual < dip_threshold, device=predictions.device, dtype=torch.float)
    
    # Calculate weighted loss for pressure only
    pressure_loss = (pred_pressure - target_pressure) ** 2
    weighted_pressure_loss = pressure_loss * (1 + 2 * dip_mask)  # 3x weight on dips
    
    # Combine losses
    total_loss = mse_loss + 0.5 * weighted_pressure_loss.mean()
    
    return total_loss


# SECTION 10 : Training function that mimics Keras fit() (matching original style)
def fit(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, verbose=2):
    train_losses = []
    val_losses = []
    
    # Use Adam optimizer like original
    optimizer = torch.optim.Adam(model.parameters())
    
    for epoch in range(epochs):
        # Training
        model.train()
        epoch_loss = 0
        num_batches = 0
        
        for i in range(0, len(X_train), batch_size):
            batch_X = X_train[i:i+batch_size]
            batch_Y = Y_train[i:i+batch_size]
            
            # Forward pass
            predictions = model(batch_X)
            loss = dip_aware_loss(predictions, batch_Y, pressure_scaler)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        avg_train_loss = epoch_loss / num_batches
        train_losses.append(avg_train_loss)
        
        # Validation
        model.eval()
        with torch.no_grad():
            val_predictions = model(X_val)
            val_loss = dip_aware_loss(val_predictions, Y_val, pressure_scaler)
            val_losses.append(val_loss.item())
        
        # Print progress (verbose=2 means one line per epoch)
        if verbose == 2:
            print(f'Epoch {epoch+1}/{epochs} - loss: {avg_train_loss:.4f} - val_loss: {val_loss.item():.4f}')
        elif verbose == 1 and (epoch + 1) % 100 == 0:
            print(f'Epoch {epoch+1}/{epochs} - loss: {avg_train_loss:.4f} - val_loss: {val_loss.item():.4f}')
    
    return train_losses, val_losses


# SECTION 11 : Create model and optimizer (matching original style)
model = SequentialLSTMModel(input_size=2, output_size=2).to(device)

# Use Adam optimizer like original
loss_function = dip_aware_loss  # Keep dip-aware loss from original

total_params = sum(p.numel() for p in model.parameters())
print(f"\nModel has {total_params:,} parameters")
print("Using dip-aware loss function with threshold -5.0 psiA")


# SECTION 12 : Train the model (using original fit function)
print("\nTraining model...")
train_losses, val_losses = fit(
    model, X_train, Y_train, X_val, Y_val,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=2
)


# SECTION 13 : Evaluate model (matching original style)
print("\nEvaluating model...")
model.eval()

with torch.no_grad():
    predictions = model(X_test).cpu().numpy()
    targets = Y_test.cpu().numpy()

# Extract pressure predictions
predictions_pressure = predictions[:, :, 1]
targets_pressure = targets[:, :, 1]

# Denormalize
predictions_actual = pressure_scaler.inverse_transform(
    predictions_pressure.reshape(-1, 1)
).reshape(predictions_pressure.shape)

targets_actual = pressure_scaler.inverse_transform(
    targets_pressure.reshape(-1, 1)
).reshape(targets_pressure.shape)

# Calculate metrics
correlation = np.corrcoef(predictions_actual.flatten(), targets_actual.flatten())[0, 1]
mse = np.mean((predictions_actual - targets_actual) ** 2)
rmse = np.sqrt(mse)
mae = np.mean(np.abs(predictions_actual - targets_actual))

# Dip detection metrics
dip_mask = targets_actual < -5.0
if dip_mask.any():
    dip_mse = np.mean((predictions_actual[dip_mask] - targets_actual[dip_mask]) ** 2)
    dip_rmse = np.sqrt(dip_mse)
    print(f"Dip regions RMSE: {dip_rmse:.2f} psiA")
    print(f"Number of dip samples: {dip_mask.sum()}")

print(f"\nOverall Metrics:")
print(f"Correlation: {correlation:.3f}")
print(f"RMSE: {rmse:.2f} psiA")
print(f"MAE: {mae:.2f} psiA")

# Fidelity scores
errors = np.abs(predictions_actual - targets_actual)
mean_error = np.mean(errors)
pressure_range = np.max(targets_actual) - np.min(targets_actual)
accuracy_score = max(0, min(100, (1 - mean_error / pressure_range) * 100))
dynamics_score = correlation * 100

print(f"\nFidelity Scores:")
print(f"Accuracy: {accuracy_score:.1f}% (value closeness)")
print(f"Dynamics: {dynamics_score:.1f}% (pattern capture)")
print(f"Overall: {(accuracy_score + dynamics_score) / 2:.1f}%")


# SECTION 13 : Visualization
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Plot 1: Training history
axes[0, 0].plot(train_losses, label='Train Loss', alpha=0.7, linewidth=2)
axes[0, 0].plot(val_losses, label='Val Loss', alpha=0.7, linewidth=2)
axes[0, 0].set_title('Training History (with Early Stopping)')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].set_yscale('log')

# Plot 2: Learning curves (zoomed)
recent_epochs = min(50, len(train_losses))
axes[0, 1].plot(train_losses[-recent_epochs:], label='Train Loss', linewidth=2)
axes[0, 1].plot(val_losses[-recent_epochs:], label='Val Loss', linewidth=2)
axes[0, 1].set_title(f'Last {recent_epochs} Epochs (Detail)')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Time series predictions
n_show = min(200, len(targets_actual))
axes[0, 2].plot(targets_actual[:n_show, 0], 'b-', label='Actual', linewidth=2)
axes[0, 2].plot(predictions_actual[:n_show, 0], 'r-', label='Predicted', alpha=0.8)
axes[0, 2].axhline(y=-5.0, color='g', linestyle='--', alpha=0.5, label='Dip Threshold')
axes[0, 2].set_title('Pressure Predictions - First Step')
axes[0, 2].set_xlabel('Test Sample')
axes[0, 2].set_ylabel('Pressure (psiA)')
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# Plot 4: Scatter plot
axes[1, 0].scatter(targets_actual.flatten(), predictions_actual.flatten(), 
                   alpha=0.3, s=1, c='blue')
if dip_mask.any():
    axes[1, 0].scatter(targets_actual.flatten()[dip_mask.flatten()], 
                       predictions_actual.flatten()[dip_mask.flatten()], 
                       alpha=0.8, s=2, c='red', label='Dip regions')
min_val = min(targets_actual.min(), predictions_actual.min())
max_val = max(targets_actual.max(), predictions_actual.max())
axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, alpha=0.5)
axes[1, 0].axvline(x=-5.0, color='g', linestyle='--', alpha=0.5)
axes[1, 0].set_title(f'Prediction Accuracy (R={correlation:.3f})')
axes[1, 0].set_xlabel('Actual Pressure (psiA)')
axes[1, 0].set_ylabel('Predicted Pressure (psiA)')
axes[1, 0].grid(True, alpha=0.3)
if dip_mask.any():
    axes[1, 0].legend()

# Plot 5: Residuals histogram
residuals = predictions_actual.flatten() - targets_actual.flatten()
axes[1, 1].hist(residuals, bins=50, alpha=0.7, color='blue', edgecolor='black')
axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)
axes[1, 1].set_title(f'Residuals Distribution (μ={np.mean(residuals):.2f}, σ={np.std(residuals):.2f})')
axes[1, 1].set_xlabel('Prediction Error (psiA)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].grid(True, alpha=0.3)

# Plot 6: Multi-step predictions
for step in range(min(3, PRED_LENGTH)):  # Show first 3 steps
    alpha = 0.8 - step * 0.2
    axes[1, 2].plot(targets_actual[:50, step], 'b-', alpha=alpha, 
                    linewidth=2-step*0.3, label=f'Actual (t+{step+1})' if step == 0 else '')
    axes[1, 2].plot(predictions_actual[:50, step], 'r-', alpha=alpha, 
                    linewidth=2-step*0.3, label=f'Predicted (t+{step+1})' if step == 0 else '')

axes[1, 2].axhline(y=-5.0, color='g', linestyle='--', alpha=0.5, label='Dip Threshold')
axes[1, 2].set_title(f'Multi-Step Ahead Predictions')
axes[1, 2].set_xlabel('Test Sample')
axes[1, 2].set_ylabel('Pressure (psiA)')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("Model training complete!")
print("Key difference from original: TEMPORAL DATA SPLITTING")
print("This prevents future data from leaking into past predictions")
print("Everything else matches your original code structure")
print("="*60)
