import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler  # Changed from StandardScaler
from sklearn.metrics import accuracy_score
from torchvision.models import inception_v3
from scipy.stats import pearsonr


# Basic settings - Reduced complexity to prevent overfitting
SEQ_LENGTH = 20  
PRED_LENGTH = 5  
HIDDEN_SIZE = 64    # Reduced from 128
DENSE_SIZE = 64     # Reduced from 128
BATCH_SIZE = 170
LEARNING_RATE = 5e-4  # Keep original learning rate
EPOCHS = 1000       # Reduced from 3000, early stopping will handle this


# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# Load data
print("Loading data...")
df = pd.read_excel('')
flows = df['Flow Rate'].values
pressures = df['Pressure'].values


# FIXED: Separate normalization like All_Models.py
scaler_flow = MinMaxScaler(feature_range=(-1, 1))
scaler_pressure = MinMaxScaler(feature_range=(-1, 1))

flows_normalized = scaler_flow.fit_transform(flows.reshape(-1, 1)).flatten()
pressures_normalized = scaler_pressure.fit_transform(pressures.reshape(-1, 1)).flatten()

# Combine flow and pressure data
data_normalized = np.column_stack([flows_normalized, pressures_normalized])


# FIXED: Create non-overlapping sequences to prevent data leakage
print("Creating sequences...")
def create_sequences_no_overlap(data, seq_length, pred_length):
    """Create non-overlapping sequences to prevent data leakage"""
    X_data = []
    Y_data = []
    
    step_size = seq_length  # Non-overlapping
    for i in range(0, len(data) - seq_length - pred_length + 1, step_size):
        if i + seq_length + pred_length <= len(data):
            input_seq = data[i:i+seq_length]
            X_data.append(input_seq)
            
            target_pressures = data[i+seq_length:i+seq_length+pred_length, 1]
            Y_data.append(target_pressures)
    
    return np.array(X_data), np.array(Y_data)

X_data, Y_data = create_sequences_no_overlap(data_normalized, SEQ_LENGTH, PRED_LENGTH)

print(f"Created {len(X_data)} non-overlapping sequences")


# FIXED: Proper data splitting with shuffling
print("Splitting data...")
# Shuffle data before splitting
indices = np.random.permutation(len(X_data))
X_data_shuffled = X_data[indices]
Y_data_shuffled = Y_data[indices]

# Split into train and test 
split = int(.8 * len(X_data_shuffled))
X_train = torch.FloatTensor(X_data_shuffled[:split])
Y_train = torch.FloatTensor(Y_data_shuffled[:split])  # FIXED: Correct slice
X_test = torch.FloatTensor(X_data_shuffled[split:])
Y_test = torch.FloatTensor(Y_data_shuffled[split:])   # FIXED: Correct slice

print(f"Training samples: {len(X_train)}")
print(f"Test Samples: {len(X_test)}")


# FIXED: Define Model with corrected architecture
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

        # LSTM layer
        self.lstm = nn.LSTM(input_size=2, hidden_size=HIDDEN_SIZE, batch_first=True)

        # Dense layers (fully connected) - FIXED naming
        self.dense1 = nn.Linear(HIDDEN_SIZE, DENSE_SIZE)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(DENSE_SIZE, PRED_LENGTH)

    def forward(self, x):
        lstm_out, (hidden, cell) = self.lstm(x)
        last_hidden = hidden[-1]
        x = self.dense1(last_hidden)  # FIXED: was self.dense
        x = self.relu(x)
        predictions = self.dense2(x)
        return predictions
    
model = Model(dropout=0.3).to(device)

# loss function (MSE) and optimizer (Adam)
MSE = nn.MSELoss()
Adam = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)


# FIXED: Training with proper validation split
print("\nStarting Training...")
train_losses = []
val_losses = []

# FIXED: Create a validation set from last 20% of training data
val_split = int(0.8 * len(X_train))
X_train_sub = X_train[:val_split]
Y_train_sub = Y_train[:val_split]
X_val = X_train[val_split:]
Y_val = Y_train[val_split:]  # FIXED: was X_train[val_split:]

print(f"Training subset: {len(X_train_sub)} samples")
print(f"Validation subset: {len(X_val)} samples")

for epoch in range(EPOCHS):
    total_loss = 0
    num_batches = 0

    # Train on batches
    model.train()
    for i in range(0, len(X_train_sub), BATCH_SIZE):
        # Get batch
        batch_X = X_train_sub[i:i+BATCH_SIZE].to(device)
        batch_Y = Y_train_sub[i:i+BATCH_SIZE].to(device)

        # Forward pass
        predictions = model(batch_X)
        loss = MSE(predictions, batch_Y)

        # Backward pass
        Adam.zero_grad()
        loss.backward()
        Adam.step()

        total_loss += loss.item()
        num_batches += 1

    # FIXED: Average loss calculation
    avg_loss = total_loss / num_batches  # FIXED: was avg_loss +=
    train_losses.append(avg_loss)

    # Validation loss
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val.to(device))
        val_loss = MSE(val_outputs, Y_val.to(device))
        val_losses.append(val_loss.item())

    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}')

# Testing
print("\nTesting model...")
model.eval()

# Predictions
all_predictions = []
all_targets = []

with torch.no_grad():
    for i in range(0, len(X_test), BATCH_SIZE):
        batch_X = X_test[i:i+BATCH_SIZE].to(device)
        batch_Y = Y_test[i:i+BATCH_SIZE]

        predictions = model(batch_X)

        all_predictions.append(predictions.cpu().numpy())
        all_targets.append(batch_Y.numpy())

predictions_array = np.vstack(all_predictions)
targets_array = np.vstack(all_targets)

# FIXED: Convert values back to original scale using correct scaler
predictions_actual = scaler_pressure.inverse_transform(predictions_array.reshape(-1, 1)).reshape(predictions_array.shape)
targets_actual = scaler_pressure.inverse_transform(targets_array.reshape(-1, 1)).reshape(targets_array.shape)

# FIXED: Calculate fidelity metrics (corrected R² calculation)
predictions_flat = predictions_actual.flatten()
targets_flat = targets_actual.flatten()

ss_res = np.sum((targets_flat - predictions_flat) ** 2)  # FIXED: corrected calculation
ss_tot = np.sum((targets_flat - np.mean(targets_flat)) ** 2)
r_squared = 1 - (ss_res / ss_tot)

print(f"R-squared: {r_squared:.4f}")

# Make plots
plt.figure(figsize=(15, 5))

# Plot 1: Training and validation loss
plt.subplot(1, 3, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')  # FIXED: was plt.xlabel('Loss')
plt.legend()
plt.grid(True)

# Plot 2: Predictions vs Actual (first prediction of each sequence)
plt.subplot(1, 3, 2)
plt.plot(targets_actual[:, 0], label='Actual', alpha=0.7)
plt.plot(predictions_actual[:, 0], label='Predicted', alpha=0.7)
plt.title('Predictions vs Actual (First Step)')
plt.xlabel('Test Sample')
plt.ylabel('Pressure (psiA)')
plt.legend()
plt.grid(True)

# Plot 3: Scatter plot for correlation
plt.subplot(1, 3, 3)
plt.scatter(targets_flat, predictions_flat, alpha=0.5)
plt.plot([targets_flat.min(), targets_flat.max()], [targets_flat.min(), targets_flat.max()], 'r--', lw=2)
plt.xlabel('Actual Pressure')
plt.ylabel('Predicted Pressure')
plt.title(f'Correlation (R² = {r_squared:.4f})')
plt.grid(True)

plt.tight_layout()
plt.show()

print("\nDone!")
