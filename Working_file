import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
import copy

# Basic settings - Reduced complexity to prevent overfitting
SEQ_LENGTH = 20  
HIDDEN_SIZE = 64    # Reduced from 128
DENSE_SIZE = 64     # Reduced from 128
BATCH_SIZE = 32     # Smaller for better training
LEARNING_RATE = 1e-3
EPOCHS = 1000       # Early stopping will handle this
DROPOUT = 0.3       # Increased dropout for regularization

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Load data
print("Loading data...")
df = pd.read_excel('')
flows = df['Flow Rate'].values
pressures = df['Pressure'].values

print(f"Data points: {len(flows)}")
print(f"Flow range: {flows.min():.2f} to {flows.max():.2f}")
print(f"Pressure range: {pressures.min():.2f} to {pressures.max():.2f}")

# Separate normalization for better convergence
scaler_flow = MinMaxScaler(feature_range=(-1, 1))
scaler_pressure = MinMaxScaler(feature_range=(-1, 1))

flows_normalized = scaler_flow.fit_transform(flows.reshape(-1, 1)).flatten()
pressures_normalized = scaler_pressure.fit_transform(pressures.reshape(-1, 1)).flatten()

print(f"Normalized flow range: {flows_normalized.min():.3f} to {flows_normalized.max():.3f}")
print(f"Normalized pressure range: {pressures_normalized.min():.3f} to {pressures_normalized.max():.3f}")

# Combine data for sequence models
data_normalized = np.column_stack([flows_normalized, pressures_normalized])

# FIXED: Create non-overlapping sequences to prevent data leakage (like our final LSTM_model.py)
print("Creating sequences for LSTM models...")
def create_sequences_no_overlap(data, seq_length):
    """Create non-overlapping sequences to prevent data leakage"""
    X_data = []
    Y_data = []
    
    step_size = seq_length  # Non-overlapping
    for i in range(0, len(data) - seq_length, step_size):
        if i + seq_length < len(data):
            input_seq = data[i:i+seq_length]  # Both flow and pressure history
            X_data.append(input_seq)
            
            target_pressure = data[i+seq_length, 1]  # Next pressure value
            Y_data.append(target_pressure)
    
    return np.array(X_data), np.array(Y_data)

X_seq, Y_seq = create_sequences_no_overlap(data_normalized, SEQ_LENGTH)
print(f"Created {len(X_seq)} sequences")

# Create direct mapping data for FNN (flow -> pressure)
print("Creating direct mapping for FNN...")
X_direct = flows_normalized.reshape(-1, 1)
Y_direct = pressures_normalized

print(f"Direct mapping: {len(X_direct)} samples")

# FIXED: Create flow-only sequences with non-overlapping approach (for RNN models)
print("Creating flow-only sequences...")
flow_seq = []
pressure_targets = []
step_size = SEQ_LENGTH  # Non-overlapping

for i in range(0, len(flows_normalized) - SEQ_LENGTH, step_size):
    if i + SEQ_LENGTH < len(flows_normalized):
        flow_seq.append(flows_normalized[i:i+SEQ_LENGTH])
        pressure_targets.append(pressures_normalized[i+SEQ_LENGTH])

flow_seq = np.array(flow_seq)
pressure_targets = np.array(pressure_targets)

print(f"Flow-only sequences: {len(flow_seq)} samples")

# Data splitting function (same as LSTM_model.py)
def split_data(X, Y, test_size=0.2):
    indices = np.random.permutation(len(X))
    X_shuffled = X[indices]
    Y_shuffled = Y[indices]
    
    split = int((1 - test_size) * len(X_shuffled))
    X_train = torch.FloatTensor(X_shuffled[:split])
    Y_train = torch.FloatTensor(Y_shuffled[:split])
    X_test = torch.FloatTensor(X_shuffled[split:])
    Y_test = torch.FloatTensor(Y_shuffled[split:])
    
    return X_train, Y_train, X_test, Y_test

# Model Definitions (simplified, no batch norm)

# 1. Simple FNN Model (Direct mapping: flow -> pressure)
class FNNModel(nn.Module):
    def __init__(self, dropout=DROPOUT):
        super(FNNModel, self).__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(1, HIDDEN_SIZE * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(HIDDEN_SIZE * 2, HIDDEN_SIZE),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(HIDDEN_SIZE // 2, 1)
        )
        
    def forward(self, x):
        return self.layers(x)

# 2. LSTM-Dense-Dense Model (Uses flow+pressure sequences -> pressure)
class LSTMDenseDenseModel(nn.Module):
    def __init__(self, dropout=DROPOUT):
        super(LSTMDenseDenseModel, self).__init__()
        
        # LSTM layer
        self.lstm = nn.LSTM(input_size=2, hidden_size=HIDDEN_SIZE, batch_first=True)

        # Dense layers (fully connected) - FIXED naming
        self.dense1 = nn.Linear(HIDDEN_SIZE, DENSE_SIZE)
        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        self.dense2 = nn.Linear(DENSE_SIZE, 1)

    def forward(self, x):
        lstm_out, (hidden, cell) = self.lstm(x)
        last_hidden = hidden[-1]
        
        x = self.dense1(last_hidden)  # FIXED: was self.dense
        x = self.relu(x)
        x = self.dropout1(x)
        predictions = self.dense2(x)
        return predictions

# 3. Dense-RNN-Dense Model (Uses flow sequences -> pressure)
class DenseRNNDenseModel(nn.Module):
    def __init__(self, dropout=DROPOUT):
        super(DenseRNNDenseModel, self).__init__()
        
        self.rnn = nn.RNN(1, HIDDEN_SIZE, batch_first=True)
        self.dense1 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE // 2)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        self.dense2 = nn.Linear(HIDDEN_SIZE // 2, 1)
        
    def forward(self, x):
        # x shape: (batch_size, seq_length)
        x = x.unsqueeze(-1)  # Add feature dimension
        
        rnn_out, _ = self.rnn(x)
        x = rnn_out[:, -1, :]  # Take last output
        
        x = self.dense1(x)
        x = self.relu1(x)
        x = self.dropout1(x)
        x = self.dense2(x)
        
        return x

# 4. Deep Neural Network Model (Direct mapping: flow -> pressure, deeper architecture)
class DeepNNModel(nn.Module):
    def __init__(self, dropout=DROPOUT):
        super(DeepNNModel, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(1, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 96),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(96, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        return self.network(x)

# Training function (same style as LSTM_model.py with ALL fixes)
def train_model(model, X_train, Y_train, X_test, Y_test, model_name, epochs=EPOCHS):
    print(f"\nTraining {model_name}...")
    
    # Loss function and optimizer (same as LSTM_model.py)
    MSE = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    train_losses = []
    val_losses = []
    
    # Create validation set (same as LSTM_model.py)
    val_split = int(0.8 * len(X_train))
    X_train_sub = X_train[:val_split]
    Y_train_sub = Y_train[:val_split]
    X_val = X_train[val_split:]
    Y_val = Y_train[val_split:]
    
    print(f"Training subset: {len(X_train_sub)} samples")
    print(f"Validation subset: {len(X_val)} samples")
    
    # Early stopping parameters
    best_val_loss = float('inf')
    patience = 50  # Same as our final LSTM version
    patience_counter = 0
    best_model_state = None
    
    for epoch in range(epochs):
        total_loss = 0
        num_batches = 0

        # FIXED: Set model to training mode
        model.train()
        
        # FIXED: Train on batches using ONLY the training subset (not full X_train)
        for i in range(0, len(X_train_sub), BATCH_SIZE):
            batch_X = X_train_sub[i:i+BATCH_SIZE].to(device)
            batch_Y = Y_train_sub[i:i+BATCH_SIZE].to(device)

            # Forward pass
            predictions = model(batch_X).squeeze()
            loss = MSE(predictions, batch_Y)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        # Average loss
        avg_loss = total_loss / num_batches
        train_losses.append(avg_loss)

        # FIXED: Validation loss
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val.to(device)).squeeze()
            val_loss = MSE(val_outputs, Y_val.to(device))
            val_losses.append(val_loss.item())
        
        # Early stopping check
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = copy.deepcopy(model.state_dict())
        else:
            patience_counter += 1
        
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

        if (epoch + 1) % 50 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience_counter}/{patience}')

    # Restore best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"Restored best model with validation loss: {best_val_loss:.4f}")
    else:
        print("No improvement found, using final model state")
    
    # Test the model
    model.eval()
    with torch.no_grad():
        test_predictions = model(X_test.to(device)).squeeze().cpu().numpy()
        test_targets = Y_test.cpu().numpy()

    # FIXED: Convert back to original scale using correct scaler
    predictions_actual = scaler_pressure.inverse_transform(test_predictions.reshape(-1, 1)).flatten()
    targets_actual = scaler_pressure.inverse_transform(test_targets.reshape(-1, 1)).flatten()

    # FIXED: Calculate fidelity metrics (corrected RÂ² calculation)
    r2 = r2_score(targets_actual, predictions_actual)
    mse = np.mean((targets_actual - predictions_actual) ** 2)
    mae = np.mean(np.abs(targets_actual - predictions_actual))
    
    print(f"{model_name} - RÂ²: {r2:.4f}, MSE: {mse:.2f}, MAE: {mae:.2f}")
    
    return r2, mse, predictions_actual, targets_actual, train_losses, val_losses

# Main training and comparison
print("\n" + "="*60)
print("MODEL COMPARISON")
print("="*60)

# ðŸ“Š NOW COMPARING 5 MODELS:

# 1. **FNN** - Direct flow â†’ pressure mapping
# 2. **LSTM-Dense-Dense** - Flow+pressure sequences â†’ pressure (your original fixed model)
# 3. **Dense-RNN-Dense** - Flow sequences â†’ pressure (RNN version) 
# 4. **Dense-LSTM-Dense** - Flow sequences â†’ pressure (LSTM version)
# 5. **Deep NN** - Direct flow â†’ pressure mapping (deeper network)

results = {}

# 1. Train FNN Model (uses direct mapping)
X_train_direct, Y_train_direct, X_test_direct, Y_test_direct = split_data(X_direct, Y_direct)
fnn_model = FNNModel().to(device)
fnn_r2, fnn_mse, fnn_pred, fnn_targets, fnn_train_loss, fnn_val_loss = train_model(
    fnn_model, X_train_direct, Y_train_direct, X_test_direct, Y_test_direct, "FNN"
)
results['FNN'] = {
    'r2': fnn_r2, 'mse': fnn_mse, 'pred': fnn_pred, 'targets': fnn_targets,
    'train_losses': fnn_train_loss, 'val_losses': fnn_val_loss
}

# 2. Train LSTM Model (uses sequences)
X_train_seq, Y_train_seq, X_test_seq, Y_test_seq = split_data(X_seq, Y_seq)
lstm_model = LSTMModel().to(device)
lstm_r2, lstm_mse, lstm_pred, lstm_targets, lstm_train_loss, lstm_val_loss = train_model(
    lstm_model, X_train_seq, Y_train_seq, X_test_seq, Y_test_seq, "LSTM"
)
results['LSTM'] = {
    'r2': lstm_r2, 'mse': lstm_mse, 'pred': lstm_pred, 'targets': lstm_targets,
    'train_losses': lstm_train_loss, 'val_losses': lstm_val_loss
}

# 3. Train Dense-LSTM-Dense Model (uses flow sequences only with non-overlapping)
# FIXED: Create flow-only sequences with non-overlapping approach
flow_seq = []
pressure_targets = []
step_size = SEQ_LENGTH  # Non-overlapping

for i in range(0, len(flows_normalized) - SEQ_LENGTH, step_size):
    if i + SEQ_LENGTH < len(flows_normalized):
        flow_seq.append(flows_normalized[i:i+SEQ_LENGTH])
        pressure_targets.append(pressures_normalized[i+SEQ_LENGTH])

flow_seq = np.array(flow_seq)
pressure_targets = np.array(pressure_targets)

X_train_flow, Y_train_flow, X_test_flow, Y_test_flow = split_data(flow_seq, pressure_targets)
dense_lstm_model = DenseLSTMDenseModel().to(device)
dlstm_r2, dlstm_mse, dlstm_pred, dlstm_targets, dlstm_train_loss, dlstm_val_loss = train_model(
    dense_lstm_model, X_train_flow, Y_train_flow, X_test_flow, Y_test_flow, "Dense-LSTM-Dense"
)
results['Dense-LSTM-Dense'] = {
    'r2': dlstm_r2, 'mse': dlstm_mse, 'pred': dlstm_pred, 'targets': dlstm_targets,
    'train_losses': dlstm_train_loss, 'val_losses': dlstm_val_loss
}

# 4. Train Deep NN Model (uses direct mapping)
deep_nn_model = DeepNNModel().to(device)
deep_r2, deep_mse, deep_pred, deep_targets, deep_train_loss, deep_val_loss = train_model(
    deep_nn_model, X_train_direct, Y_train_direct, X_test_direct, Y_test_direct, "Deep NN"
)
results['Deep NN'] = {
    'r2': deep_r2, 'mse': deep_mse, 'pred': deep_pred, 'targets': deep_targets,
    'train_losses': deep_train_loss, 'val_losses': deep_val_loss
}

# Print comparison results
print("\n" + "="*70)
print("FINAL RESULTS")
print("="*70)
print(f"{'Model':<20} {'RÂ²':<10} {'MSE':<10}")
print("-"*50)

for model_name, result in results.items():
    print(f"{model_name:<20} {result['r2']:<10.4f} {result['mse']:<10.2f}")

# Find best model
best_model_name = max(results.items(), key=lambda x: x[1]['r2'])[0]
best_r2 = results[best_model_name]['r2']
print(f"\nBest model: {best_model_name} with RÂ² = {best_r2:.4f}")

# Model comparison insights
print(f"\nðŸ“Š MODEL ARCHITECTURE DETAILS:")
print(f"âœ… FNN: Direct mapping (flow â†’ pressure)")
print(f"âœ… LSTM-Dense-Dense: Sequences of [flow+pressure] â†’ pressure")  
print(f"âœ… Dense-RNN-Dense: Sequences of [flow only] â†’ pressure")
print(f"âœ… Deep NN: Direct mapping (flow â†’ pressure) with deeper layers")
print(f"\nðŸ”§ All models include: early stopping, dropout, proper data splits, non-overlapping sequences")

# Create plots (same style as our fixed LSTM_model.py with correlation plot)
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Plot 1: RÂ² comparison
model_names = list(results.keys())
r2_values = [results[name]['r2'] for name in model_names]

axes[0, 0].bar(model_names, r2_values, alpha=0.7)
axes[0, 0].set_ylabel('RÂ² Score')
axes[0, 0].set_title('Model Performance Comparison')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(True, alpha=0.3)

# Add value labels on bars
for i, (name, value) in enumerate(zip(model_names, r2_values)):
    axes[0, 0].text(i, value + 0.01, f'{value:.3f}', ha='center', va='bottom', fontsize=9)

# Plot 2: Best model predictions vs actual (time series style)
best_result = results[best_model_name]
axes[0, 1].plot(best_result['targets'], label='Actual', alpha=0.7, marker='o', markersize=4)
axes[0, 1].plot(best_result['pred'], label='Predicted', alpha=0.7, marker='s', markersize=4)
axes[0, 1].set_xlabel('Test Sample')
axes[0, 1].set_ylabel('Pressure')
axes[0, 1].set_title(f'Best Model: {best_model_name} - Predictions vs Actual')
axes[0, 1].legend()
axes[0, 1].grid(True)

# Plot 3: Best model correlation scatter plot (like our fixed LSTM_model.py)
axes[1, 0].scatter(best_result['targets'], best_result['pred'], alpha=0.6, s=50)
axes[1, 0].plot([best_result['targets'].min(), best_result['targets'].max()], 
                [best_result['targets'].min(), best_result['targets'].max()], 'r--', lw=2)
axes[1, 0].set_xlabel('Actual Pressure')
axes[1, 0].set_ylabel('Predicted Pressure')
axes[1, 0].set_title(f'Best Model Correlation (RÂ² = {best_r2:.4f})')
axes[1, 0].grid(True)

# Add RÂ² text box like our fixed LSTM_model.py
axes[1, 0].text(0.05, 0.95, f'RÂ² = {best_r2:.4f}', 
                transform=axes[1, 0].transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

# Plot 4: Training curves for best model
axes[1, 1].plot(best_result['train_losses'], label='Train Loss')
axes[1, 1].plot(best_result['val_losses'], label='Val Loss')
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('Loss')  # FIXED: was xlabel
axes[1, 1].set_title(f'{best_model_name} Training and Validation Loss')
axes[1, 1].legend()
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()

print("\nDone!")
