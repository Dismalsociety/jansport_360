import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
import copy

# Basic settings
HIDDEN_SIZES = [128, 96, 64, 32]  # Deep network layer sizes
BATCH_SIZE = 32                   # Smaller batch size for better training
LEARNING_RATE = 1e-3
EPOCHS = 1000
DROPOUT = 0.2

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Load data
print("Loading data...")
df = pd.read_excel('')
flows = df['Flow Rate'].values
pressures = df['Pressure'].values

print(f"Data points: {len(flows)}")
print(f"Flow range: {flows.min():.2f} to {flows.max():.2f}")
print(f"Pressure range: {pressures.min():.2f} to {pressures.max():.2f}")

# Separate normalization for better convergence
scaler_flow = MinMaxScaler(feature_range=(-1, 1))
scaler_pressure = MinMaxScaler(feature_range=(-1, 1))

flows_normalized = scaler_flow.fit_transform(flows.reshape(-1, 1)).flatten()
pressures_normalized = scaler_pressure.fit_transform(pressures.reshape(-1, 1)).flatten()

print(f"Normalized flow range: {flows_normalized.min():.3f} to {flows_normalized.max():.3f}")
print(f"Normalized pressure range: {pressures_normalized.min():.3f} to {pressures_normalized.max():.3f}")

# Simple direct mapping: flow -> pressure (no sequences)
print("Preparing data...")
X_data = flows_normalized.reshape(-1, 1)  # Each flow value as input
Y_data = pressures_normalized             # Corresponding pressure as target

print(f"Input shape: {X_data.shape}")
print(f"Target shape: {Y_data.shape}")

# Proper data splitting with shuffling
print("Splitting data...")
indices = np.random.permutation(len(X_data))
X_data_shuffled = X_data[indices]
Y_data_shuffled = Y_data[indices]

# Split into train and test 
split = int(.8 * len(X_data_shuffled))
X_train = torch.FloatTensor(X_data_shuffled[:split])
Y_train = torch.FloatTensor(Y_data_shuffled[:split])
X_test = torch.FloatTensor(X_data_shuffled[split:])
Y_test = torch.FloatTensor(Y_data_shuffled[split:])

print(f"Training samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")

# Define Deep Neural Network Model
class DeepNN(nn.Module):
    def __init__(self, hidden_sizes=HIDDEN_SIZES, dropout=DROPOUT):
        super(DeepNN, self).__init__()
        
        layers = []
        input_size = 1  # Single flow input
        
        # Create deep network layers
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(input_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            input_size = hidden_size
        
        # Output layer
        layers.append(nn.Linear(input_size, 1))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

model = DeepNN().to(device)

# Count and display model parameters
total_params = sum(p.numel() for p in model.parameters())
print(f"Model parameters: {total_params:,}")

# Loss function and optimizer
MSE = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)

# Training with early stopping
print("\nStarting Training...")
train_losses = []
val_losses = []

# Create validation set from last 20% of training data
val_split = int(0.8 * len(X_train))
X_train_sub = X_train[:val_split]
Y_train_sub = Y_train[:val_split]
X_val = X_train[val_split:]
Y_val = Y_train[val_split:]

print(f"Training subset: {len(X_train_sub)} samples")
print(f"Validation subset: {len(X_val)} samples")

# Early stopping parameters
best_val_loss = float('inf')
patience = 100  # Increased patience for better training
patience_counter = 0
best_model_state = None

for epoch in range(EPOCHS):
    total_loss = 0
    num_batches = 0

    # Set model to training mode
    model.train()
    
    # Train on batches using ONLY the training subset
    for i in range(0, len(X_train_sub), BATCH_SIZE):
        # Get batch from training subset only
        batch_X = X_train_sub[i:i+BATCH_SIZE].to(device)
        batch_Y = Y_train_sub[i:i+BATCH_SIZE].to(device)

        # Forward pass
        predictions = model(batch_X).squeeze()
        loss = MSE(predictions, batch_Y)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping for stable training
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    # Average loss
    avg_loss = total_loss / num_batches
    train_losses.append(avg_loss)

    # Validation loss
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val.to(device)).squeeze()
        val_loss = MSE(val_outputs, Y_val.to(device))
        val_losses.append(val_loss.item())
    
    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        best_model_state = copy.deepcopy(model.state_dict())
    else:
        patience_counter += 1
    
    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

    if (epoch + 1) % 20 == 0:
        print(f'Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience_counter}/{patience}')

# Restore best model
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    print(f"Restored best model with validation loss: {best_val_loss:.4f}")

# Testing
print("\nTesting model...")
model.eval()

# Predictions on test set
with torch.no_grad():
    test_predictions = model(X_test.to(device)).squeeze().cpu().numpy()
    test_targets = Y_test.cpu().numpy()

# Convert values back to original scale using correct scalers
predictions_actual = scaler_pressure.inverse_transform(test_predictions.reshape(-1, 1)).flatten()
targets_actual = scaler_pressure.inverse_transform(test_targets.reshape(-1, 1)).flatten()

# Calculate performance metrics
r2 = r2_score(targets_actual, predictions_actual)
mse = np.mean((targets_actual - predictions_actual) ** 2)
mae = np.mean(np.abs(targets_actual - predictions_actual))

print(f"R²: {r2:.4f}")
print(f"MSE: {mse:.2f}")
print(f"MAE: {mae:.2f}")

# Additional debug information
print(f"\nPredictions range: {predictions_actual.min():.2f} to {predictions_actual.max():.2f}")
print(f"Targets range: {targets_actual.min():.2f} to {targets_actual.max():.2f}")

# Create plots
plt.figure(figsize=(15, 5))

# Plot 1: Training and validation loss
plt.subplot(1, 3, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Plot 2: Predictions vs Actual
plt.subplot(1, 3, 2)
plt.plot(targets_actual, label='Actual', alpha=0.7, marker='o', markersize=4)
plt.plot(predictions_actual, label='Predicted', alpha=0.7, marker='s', markersize=4)
plt.title('Predictions vs Actual')
plt.xlabel('Test Sample')
plt.ylabel('Pressure')
plt.legend()
plt.grid(True)

# Plot 3: Correlation scatter plot
plt.subplot(1, 3, 3)
plt.scatter(targets_actual, predictions_actual, alpha=0.6, s=50)
plt.plot([targets_actual.min(), targets_actual.max()], 
         [targets_actual.min(), targets_actual.max()], 'r--', lw=2)
plt.xlabel('Actual Pressure')
plt.ylabel('Predicted Pressure')
plt.title(f'Correlation (R² = {r2:.4f})')
plt.grid(True)

# Add perfect prediction line and R² text
plt.text(0.05, 0.95, f'R² = {r2:.4f}\nMSE = {mse:.1f}\nMAE = {mae:.1f}', 
         transform=plt.gca().transAxes, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

plt.tight_layout()
plt.show()

print("\nDone!")

# Optional: Save the model
# torch.save(model.state_dict(), 'deep_nn_flow_pressure_model.pth')
# print("Model saved as 'deep_nn_flow_pressure_model.pth'")
