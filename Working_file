import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
import copy

# Basic settings
SEQ_LENGTH = 20  
HIDDEN_SIZE = 96
BATCH_SIZE = 170
LEARNING_RATE = 1e-3
EPOCHS = 1000
DROPOUT = 0.3

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load data
print("Loading data...")
df = pd.read_excel('')
flows = df['Flow Rate'].values
pressures = df['Pressure'].values

# Separate normalization for better convergence
scaler_flow = MinMaxScaler(feature_range=(-1, 1))
scaler_pressure = MinMaxScaler(feature_range=(-1, 1))

flows_normalized = scaler_flow.fit_transform(flows.reshape(-1, 1)).flatten()
pressures_normalized = scaler_pressure.fit_transform(pressures.reshape(-1, 1)).flatten()

print(f"Normalized flow range: {flows_normalized.min():.3f} to {flows_normalized.max():.3f}")
print(f"Normalized pressure range: {pressures_normalized.min():.3f} to {pressures_normalized.max():.3f}")

# Combine data
data_normalized = np.column_stack([flows_normalized, pressures_normalized])

# Create non-overlapping sequences for X (flow history) -> y (next pressure)
print("Creating sequences...")
def create_sequences_no_overlap(data, seq_length):
    """Create non-overlapping sequences: flow sequences -> next pressure"""
    X_data = []
    Y_data = []
    
    step_size = seq_length
    for i in range(0, len(data) - seq_length, step_size):
        if i + seq_length < len(data):
            # Input: sequence of flow rates only
            input_seq = data[i:i+seq_length, 0]  # Only flow column
            X_data.append(input_seq)
            
            # Target: next pressure value
            target_pressure = data[i+seq_length, 1]  # Next pressure
            Y_data.append(target_pressure)
    
    return np.array(X_data), np.array(Y_data)

X_data, Y_data = create_sequences_no_overlap(data_normalized, SEQ_LENGTH)
print(f"Created {len(X_data)} non-overlapping sequences")
print(f"X shape: {X_data.shape}, Y shape: {Y_data.shape}")

# Proper data splitting with shuffling
print("Splitting data...")
indices = np.random.permutation(len(X_data))
X_data_shuffled = X_data[indices]
Y_data_shuffled = Y_data[indices]

# Split into train and test 
split = int(.8 * len(X_data_shuffled))
X_train = torch.FloatTensor(X_data_shuffled[:split])
Y_train = torch.FloatTensor(Y_data_shuffled[:split])
X_test = torch.FloatTensor(X_data_shuffled[split:])
Y_test = torch.FloatTensor(Y_data_shuffled[split:])

print(f"Training samples: {len(X_train)}")
print(f"Test Samples: {len(X_test)}")

# Simple FNN Model (like All_Models.py)
class SimpleFNN(nn.Module):
    def __init__(self, input_size=SEQ_LENGTH, hidden_size=96, dropout=0.3):
        super(SimpleFNN, self).__init__()
        
        self.network = nn.Sequential(
            # First layer
            nn.Linear(input_size, hidden_size * 2),
            nn.BatchNorm1d(hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            # Second layer
            nn.Linear(hidden_size * 2, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            # Third layer
            nn.Linear(hidden_size, hidden_size // 2),
            nn.BatchNorm1d(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout / 2),
            
            # Output layer
            nn.Linear(hidden_size // 2, 1)
        )
        
    def forward(self, x):
        return self.network(x)

model = SimpleFNN(input_size=SEQ_LENGTH, hidden_size=HIDDEN_SIZE, dropout=DROPOUT).to(device)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Training with early stopping
print("\nStarting Training...")
train_losses = []
val_losses = []

# Create validation set
val_split = int(0.8 * len(X_train))
X_train_sub = X_train[:val_split]
Y_train_sub = Y_train[:val_split]
X_val = X_train[val_split:]
Y_val = Y_train[val_split:]

print(f"Training subset: {len(X_train_sub)} samples")
print(f"Validation subset: {len(X_val)} samples")

# Early stopping parameters
best_val_loss = float('inf')
patience = 50
patience_counter = 0
best_model_state = None

for epoch in range(EPOCHS):
    total_loss = 0
    num_batches = 0

    # Training
    model.train()
    for i in range(0, len(X_train_sub), BATCH_SIZE):
        batch_X = X_train_sub[i:i+BATCH_SIZE].to(device)
        batch_Y = Y_train_sub[i:i+BATCH_SIZE].to(device)

        # Forward pass
        predictions = model(batch_X).squeeze()
        loss = criterion(predictions, batch_Y)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    avg_loss = total_loss / num_batches
    train_losses.append(avg_loss)

    # Validation
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val.to(device)).squeeze()
        val_loss = criterion(val_outputs, Y_val.to(device))
        val_losses.append(val_loss.item())
    
    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        best_model_state = copy.deepcopy(model.state_dict())
    else:
        patience_counter += 1
    
    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience_counter}/{patience}')

# Restore best model
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    print(f"Restored best model with validation loss: {best_val_loss:.4f}")

# Testing
print("\nTesting model...")
model.eval()

with torch.no_grad():
    test_predictions = model(X_test.to(device)).squeeze().cpu().numpy()
    test_targets = Y_test.cpu().numpy()

# Denormalize
predictions_actual = scaler_pressure.inverse_transform(test_predictions.reshape(-1, 1)).flatten()
targets_actual = scaler_pressure.inverse_transform(test_targets.reshape(-1, 1)).flatten()

# Calculate metrics
r2 = r2_score(targets_actual, predictions_actual)
mse = np.mean((targets_actual - predictions_actual) ** 2)
mae = np.mean(np.abs(targets_actual - predictions_actual))

print(f"R²: {r2:.4f}")
print(f"MSE: {mse:.4f}")
print(f"MAE: {mae:.4f}")

# Create plots
plt.figure(figsize=(15, 5))

# Plot 1: Training curves
plt.subplot(1, 3, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Plot 2: Predictions vs Actual
plt.subplot(1, 3, 2)
plt.plot(targets_actual, label='Actual', alpha=0.7)
plt.plot(predictions_actual, label='Predicted', alpha=0.7)
plt.title('Predictions vs Actual')
plt.xlabel('Test Sample')
plt.ylabel('Pressure')
plt.legend()
plt.grid(True)

# Plot 3: Correlation
plt.subplot(1, 3, 3)
plt.scatter(targets_actual, predictions_actual, alpha=0.6)
plt.plot([targets_actual.min(), targets_actual.max()], 
         [targets_actual.min(), targets_actual.max()], 'r--', lw=2)
plt.xlabel('Actual Pressure')
plt.ylabel('Predicted Pressure')
plt.title(f'Correlation (R² = {r2:.4f})')
plt.grid(True)

plt.tight_layout()
plt.show()

print("\nDone!")
